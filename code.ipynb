{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world projects stand and fall with the quality of the data that is being used. Quality over quantity is the name of the game! For numerical or categorical data, there are many good techniques to determine and inspect the quality of data. But how can we do this for text data, where there are no set benchmarks of what makes a good and high quality text? Turns out there are some useful libraries and techniques that we can use to find out if our text data is any good! \n",
    "\n",
    "First off, we are going to use the \"textstat\" library, a fantastic tool that offers many functionalities to get a better text understanding. \n",
    "\n",
    "Also, all these snippets are taken from our open-source NLP content library. You can find all of this and more at: https://bricks.kern.ai/home :-) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring data quality for text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kiefer (2019) suggests these quality indicators for text data:\n",
    "- percentage of abbraviations\n",
    "- percentage of spelling mistakes \n",
    "- lexical diversity \n",
    "- percentage of uppercase words \n",
    "- percentage of ungrammatical sentences\n",
    "- average sentence length \n",
    "\n",
    "Let's have a look at how we can measure or improve these features of our text data. \n",
    "\n",
    "Source: https://btw.informatik.uni-rostock.de/download/workshopband/C2-5.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexical diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.794\n",
      "0.298\n"
     ]
    }
   ],
   "source": [
    "# lexical diversity \n",
    "def lexical_diversity(text):\n",
    "    word_count = len(text.split())\n",
    "    vocab_size = len(set(text.split()))\n",
    "    return round(vocab_size / word_count, 3) # this is the diversity score\n",
    "\n",
    "texts = [\n",
    "# NYT article:\n",
    "\"At Microsoft, Satya Nadella, the tech giant’s chief executive, said that Mr. Altman would be chief executive of the new research lab, “setting a new pace for innovation,” in an apparent contrast at the OpenAI board’s desire for caution in developing A.I. technology. Mr. Nadella noted in a post to X, formerly known as Twitter, that Mr. Altman’s new group will operate as an independent entity within Microsoft.\",\n",
    "# Poem for children:\n",
    "\"The wheels on the bus go round and round, Round and round, Round and round. The wheels on the bus go round and round, All through the town. The wipers on the bus go Swish, swish, swish; Swish, swish, swish; Swish, swish, swish. The wipers on the bus go Swish, swish, swish, All through the town. The horn on the bus goes Beep, beep, beep; Beep, beep, beep; Beep, beep, beep. The horn on the bus goes Beep, beep, beep, All through the town.\"\n",
    "]\n",
    "\n",
    "for t in texts:\n",
    "    print(lexical_diversity(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would argue that a higher lexical diversity is pretty useful to get an understanding of how complicated a text is. A text with a super hight lexical diversity might be too confusing for an LLM (or non-expert human readers) and might require you to provide an glossary or to simplify the language that is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence complexity is another great indicator, as sentences that are too complex are often not good when doing RAG. \n",
    "\n",
    "**The formula for this is as follows: 206.835 – (1.015 x Average Sentence Length) – (84.6 x Average Syllables Per Word)** <br>\n",
    "You can also find a weighted version of this code here: https://bricks.kern.ai/classifiers/1049 (applied some additional aggregation logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Doctors from Stockholm University invent cure for rare disease.\" is fairly difficult\n",
      "\"Mary had a little lamb.\" is very easy\n"
     ]
    }
   ],
   "source": [
    "# sentence complexity with textstat\n",
    "import textstat\n",
    "\n",
    "def sentence_complexity(text:str)->str:    \n",
    "    return lookup_label(textstat.flesch_reading_ease(text))\n",
    "\n",
    "def lookup_label(score:int) -> str:\n",
    "    if score < 30:\n",
    "        return \"very difficult\"\n",
    "    if score < 50:\n",
    "        return \"difficult\"\n",
    "    if score < 60:\n",
    "        return \"fairly difficult\"\n",
    "    if score < 70:\n",
    "        return \"standard\"\n",
    "    if score < 80:\n",
    "        return \"fairly easy\"\n",
    "    if score < 90:\n",
    "        return \"easy\"        \n",
    "    return \"very easy\"\n",
    "\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"Doctors from Stockholm University invent cure for rare disease.\", \"Mary had a little lamb.\"]\n",
    "    textstat.set_lang(\"en\") # you can use en, de, es, fr, it, nl, ru\n",
    "    for text in texts:\n",
    "        print(f\"\\\"{text}\\\" is {sentence_complexity(text)}\")\n",
    "\n",
    "example_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that sentences don't simply need to be long to be complex to be detected as difficult. <br>\n",
    "Besides that, **textstat** also offers some useful statistical features, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reding time of the text: 0.97 seconds (this is an average, some read even faster, some read slower) \n",
      "\n",
      "Amount of word with three or more syllables: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a super short text, which contains the word Epigallocatechin-gallate\"\n",
    "\n",
    "print(f\"Reding time of the text: {textstat.reading_time(text, ms_per_char=14.69)} seconds (this is an average, some read even faster, some read slower) \\n\")\n",
    "print(f\"Amount of words with three or more syllables: {textstat.polysyllabcount(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RAG, it also makes sense to get some knowledge about the length of you texts to find out how suitable they are. As we seen above, the length is also a great indicator for the quality of the text data. <br><br>\n",
    "Doing this can have two purposes: \n",
    "- Finding out the complexity of the data -> Additional indicators for if the text needs additional treatment\n",
    "- Keeping in mind the limited token and context window of an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also plays a role for the famous Lost in the Middle problem: https://arxiv.org/pdf/2307.03172.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lost in the middle by Liu et al.](lost-in-the-middle.png \"LLMs often ignore context in the middle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is short.\" is -> short\n",
      "\"This is a text with medium length.\" is -> medium\n",
      "\"This is a longer text with many more words. There is even a second sentence with extra words. Splendid, what a joyful day!\" is -> long\n"
     ]
    }
   ],
   "source": [
    "# classifiy a text based on their word count\n",
    "def word_count_classifier(text: str) -> str:\n",
    "    words = text.split()\n",
    "    length = len(words)\n",
    "    if length < 5:\n",
    "          return \"short\"\n",
    "    elif length < 20:\n",
    "          return \"medium\"\n",
    "    else:\n",
    "          return \"long\"\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"This is short.\", \"This is a text with medium length.\", \"This is a longer text with many more words. There is even a second sentence with extra words. Splendid, what a joyful day!\"]\n",
    "    for text in texts:\n",
    "        print(f\"\\\"{text}\\\" is -> {word_count_classifier(text)}\")\n",
    "\n",
    "example_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use the **tiktoken** library from OpenAI, which uses the tokenizer of the GPT models to count the amount of tokens (subwords). <br>\n",
    "(There are of course other tokenizers we can use for open-source models, too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is a short text with few tokens.\" -> 9\n",
      "\"This is a second short text\" -> 6\n"
     ]
    }
   ],
   "source": [
    "# classify a text based on their token contents\n",
    "import tiktoken\n",
    "\n",
    "def tiktoken_token_counter(text: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"This is a short text with few tokens.\", \"This is a second short text\"]\n",
    "    for text in texts:\n",
    "        print(f\"\\\"{text}\\\" -> {tiktoken_token_counter(text)}\")\n",
    "\n",
    "example_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned that text that for example contain special characters or extracted tables pose a risk of confusing an LLM and lead to unstable outputs. Let see how we could tackle this! \n",
    "\n",
    "Especially when you extract data from PDFs, the extracted contents can often contain some weird special characters. While this might not be too much of a problem for human readers (depending on how many special characters and fragments are in the text), an LLM can easily get distracted by some weird characters. Using the snippet below, we can easily detect some weird stuff in our data and make sure to quickly find data that we would need to manually check! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This contains a special char 你好.\" -> True\n",
      "\"Such a clean text, wow!\" -> False\n",
      "\"This is a greek letter: α\" -> True\n",
      "\"Super funny 😀\" -> True\n",
      "\"Rainbows are very nice.\" -> False\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from typing import List, Tuple\n",
    "\n",
    "DEFAULT_ALLOWED_RANGE = set(range(32, 127)).union( # Basic Latin\n",
    "    set(range(160, 255)), # Latin-1 Supplement\n",
    "    set(range(256, 384)),  # Latin Extended-A\n",
    "    set(range(384, 592)),  # Latin Extended-B\n",
    "    set(range(8192, 8303)),  # General Punctuation\n",
    "    set(range(8352, 8399)),  # Currency Symbols\n",
    "    set([ord(\"\\t\"), ord(\"\\n\"), ord(\"\\r\")])# common stop chars\n",
    ")\n",
    "\n",
    "\n",
    "def special_character_classifier(text: str, allowed_range: List[int] = None) -> str: \n",
    "    if allowed_range is None:\n",
    "        allowed_range= DEFAULT_ALLOWED_RANGE\n",
    "    \n",
    "    for char in text:\n",
    "        if ord(char) not in allowed_range and unicodedata.category(char) != \"Zs\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"This contains a special char 你好.\", \"Such a clean text, wow!\", \"This is a greek letter: α\", \"Super funny 😀\", \"Rainbows are very nice.\"]\n",
    "    for text in texts:\n",
    "        print(f\"\\\"{text}\\\" -> {special_character_classifier(text)}\")\n",
    "\n",
    "example_integration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting is another super important topic for RAG. If the text pieces (chunks) are too big, we might have issues finding the right texts or the context window of an LLM is filled up too fast. If you are having issues with the **retrieval** part of the RAG process, implementing chunking can provide you some quick and easy gains. <br><br>\n",
    "Here are some basic chunking techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n    This is a sentences.\\n    This too, but in another line\\n    ' ---> ['This is a sentences.', 'This too, but in another line']\n",
      "\n",
      "'This is a sentence\\nwith a newline literal!' ---> ['This is a sentence', 'with a newline literal!']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple newline splitter\n",
    "from typing import List \n",
    "\n",
    "def newline_splitter(text: str) -> List[str]:\n",
    "    splits = [t.strip() for t in text.split(\"\\n\")]\n",
    "    return [val for val in splits if len(val) > 0]\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation\n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"\"\"\n",
    "    This is a sentence.\n",
    "    This too, but in another line\n",
    "    \"\"\", \"This is a sentence\\nwith a newline literal!\"]\n",
    "    for text in texts:\n",
    "        print(f\"{repr(text)} ---> {newline_splitter(text)}\\n\")\n",
    "\n",
    "example_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy is a fantastic library for processing texts. We can also use it to detect sentences and use these to chunk the text data. SpaCy is also available in many different languages: https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\nThe quick brown fox jumps over the lazy dog.', '\\n\\n', 'This is a well-known pangram, a sentence that uses every letter of the alphabet at least once.\\n']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# spacy sentence splitter \n",
    "import spacy \n",
    "\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "\n",
    "This is a well-known pangram, a sentence that uses every letter of the alphabet at least once.\n",
    "\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "\n",
    "def spacy_splitter(spacy_doc):\n",
    "    return [str(sent) for sent in spacy_doc.sents]\n",
    "\n",
    "splits = spacy_splitter(doc)\n",
    "print(splits)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries like LangChain also apply some additional logic, for example by recursively chunking to build better text chunks. This can also be combined with the previous chunking approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'The quick brown fox jumps over the lazy dog. This is a well-known pangram, a sentence that uses ', 'every letter of the alphabet at least once. Pangrams have been used for centuries and are a ', 'fascinating aspect of the English language.\\n', 'However, not all languages have pangrams. For example, in Chinese, which uses a logographic writing ', \"system, it's impossible to create a pangram because there are thousands of characters, and a \", 'sentence containing all of them would be impractically long.\\n\\n', 'In contrast, languages with alphabets, like English, French, and German, can have pangrams. Some ', 'other examples of English pangrams include \"Pack my box with five dozen liquor jugs\" and \"How ', 'vexingly quick daft zebras jump!\"\\n', 'Pangrams are useful for testing keyboards, fonts, and other typography-related tools. They can show ', 'how each character in a font looks and whether any characters are missing or incorrectly rendered.\\n', 'So, the next time you see a sentence like \"The quick brown fox jumps over the lazy dog,\" remember ', \"that it's not just a quirky sentence. It's a tool that's been used for centuries to help us \", 'understand and improve our written language.\\n']\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# recursive splitter \n",
    "class RecursiveCharacterTextSplitter:\n",
    "    def __init__(self, separators=None, keep_separator=True, is_separator_regex=False, chunksize=50):\n",
    "        self.separators = separators if separators else [' ']\n",
    "        self.keep_separator = keep_separator\n",
    "        self.is_separator_regex = is_separator_regex\n",
    "        self.chunksize = chunksize\n",
    "\n",
    "    def split_text(self, text):\n",
    "        chunks = []\n",
    "        for i in range(0, len(text), self.chunksize):\n",
    "            chunk = text[i:i+self.chunksize]\n",
    "            for sep in self.separators:\n",
    "                if sep in chunk:\n",
    "                    parts = chunk.rsplit(sep, 1)  # Split on the last occurrence of the separator\n",
    "                    chunks.extend([parts[0] + sep] if self.keep_separator else [parts[0]])\n",
    "                    remaining_text = parts[1] + text[i+self.chunksize:]\n",
    "                    return chunks + self.split_text(remaining_text)  # Recursively split the remaining text\n",
    "            else:\n",
    "                chunks.append(chunk)\n",
    "        return chunks\n",
    "    \n",
    "# ↑ necessary function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(separators=[\"\\n\", \" \"], chunksize=100)\n",
    "text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. This is a well-known pangram, a sentence that uses every letter of the alphabet at least once. Pangrams have been used for centuries and are a fascinating aspect of the English language.\n",
    "However, not all languages have pangrams. For example, in Chinese, which uses a logographic writing system, it's impossible to create a pangram because there are thousands of characters, and a sentence containing all of them would be impractically long.\n",
    "\n",
    "In contrast, languages with alphabets, like English, French, and German, can have pangrams. Some other examples of English pangrams include \"Pack my box with five dozen liquor jugs\" and \"How vexingly quick daft zebras jump!\"\n",
    "Pangrams are useful for testing keyboards, fonts, and other typography-related tools. They can show how each character in a font looks and whether any characters are missing or incorrectly rendered.\n",
    "So, the next time you see a sentence like \"The quick brown fox jumps over the lazy dog,\" remember that it's not just a quirky sentence. It's a tool that's been used for centuries to help us understand and improve our written language.\n",
    "\"\"\"\n",
    "splits = splitter.split_text(text)\n",
    "\n",
    "print(splits)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables from PDFs can be a mess. Down below I extracted a table from one invoice I had laying around. You can see that the extracted table is super messy. We can use GPT to fix this! For this part, you need an OpenAI API key. See their quickstart for how to get started: https://platform.openai.com/docs/quickstart?context=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Datum     | Beschreibung                          | Preis       | Leistungsdatum | MwSt Betrag | MwSt  Betrag | netto     | brutto    |\n",
      "|-----------|---------------------------------------|-------------|----------------|-------------|--------------|-----------|-----------|\n",
      "| 13.11.23  | Fahrkarte Sparpreis, Nürnberg Hbf →... | 66,65 €     | 14.11.23^17 %  | (D) 4,36 €  | 62,29 €      | 66,65 €  |           |\n",
      "| 13.11.23  | Reservierung Nürnberg Hbf → Berlin...  | 0,00 €      | 14.11.        |             |              |           |           |\n",
      "| 13.11.23  | Reservierungsentgelt 2. Klasse        | 4,90 €      | 14.11.23^17 %  | (D) 0,32 €  | 4,58 €       | 4,90 €    |           |\n",
      "| Summe     | (netto)                               | 7 % (D)     | 66,87 €        |             |              |           |           |\n",
      "| zzgl. 7 % | MwSt (D)                              | 4,68 €      |                |             |              |           |           |\n",
      "| Summe     | (brutto)                              | 71,55 €     |                |             |              |           |           |\n"
     ]
    }
   ],
   "source": [
    "# cleaning up tables using GPT\n",
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_type = \"openai\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "raw_markdown_table = \"\"\"\n",
    "Datum Beschreibung Preis Leistungs-\n",
    "datum\n",
    "\n",
    "MwSt Betrag\n",
    "MwSt\n",
    "Betrag\n",
    "netto\n",
    "Betrag\n",
    "brutto\n",
    "13.11.23 Fahrkarte Sparpreis, Nürnberg\n",
    "Hbf → Berlin Hbf (tief), 2.\n",
    "Klasse, 1 Person (15-26 Jahre)\n",
    "\n",
    "66,65 € 14.11.23^17 % (D) 4,36 € 62,29 € 66,65 €\n",
    "13.11.23 Reservierung Nürnberg Hbf\n",
    "→ Berlin Hbf (tief), 1 Person\n",
    "(27-64 Jahre)\n",
    "\n",
    "0,00 € 14.11.\n",
    "13.11.23 Reservierungsentgelt 2. Klasse 4,90 € 14.11.23^17 % (D) 0,32 € 4,58 € 4,90 €\n",
    "\n",
    "Summe (netto) 7 % (D) 66,87 €\n",
    "zzgl. 7 % MwSt (D) 4,68 €\n",
    "\n",
    "Summe (brutto) 71,55 €\n",
    "\"\"\"\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": f\"\"\"\n",
    "        I extracted this table from a PDF, but it's pretty messy. Could you use this to create me a clean markdown table?\n",
    "        =====\n",
    "        Here's the raw extracted table: {raw_markdown_table}\n",
    "        =====\n",
    "        Start here: \n",
    "        \"\"\"},\n",
    "    temperature=0.0\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling multilingualism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working for a bigger organization, chances are that the data you work with is in multiple languages. Luckily there are some easy approaches that can make our life a bit easier when dealing with these use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we process any data, it is generally a good idea to find out what languages you are dealing with. For this, the **langdetect** library is fantastic, as it is super lightweight and easy to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"This is an english sentence.\" is written in en\n",
      "\"Dies ist ein Text in Deutsch.\" is written in de\n"
     ]
    }
   ],
   "source": [
    "# detect the language of your data\n",
    "from langdetect import detect\n",
    "\n",
    "def language_detection(text:str)->str:    \n",
    "    if not text or not text.strip():\n",
    "        return \"unknown\"\n",
    "    return detect(text)\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"This is an english sentence.\", \"Dies ist ein Text in Deutsch.\"]\n",
    "    for text in texts:\n",
    "        print(f\"\\\"{text}\\\" is written in {language_detection(text)}\")\n",
    "\n",
    "example_integration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often noticed that, even when given the right instructions, the answer of an LLM is often in the language that the system message is in. Using **langdetect**, we can quite easily classify an input text and then inject a predefined system messages, even if the data for our context is in a different language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![simple architecture](multilingualism.png \"Simple multilingual setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual embeddings & search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually processing the text data is the next logical step. While the **text-embedding-ada-002** embedding model from OpenAI is really good a languages like English, German or Mandarin, the performance is lacking for minority languages.<br> For smaller languages, we can use a model like **intfloat/multilingual-e5-small** to embed out texts. The model is lightweight enough to run locally, even on normal hardware / CPU only.<br><br>\n",
    "Link on HuggingFace: https://huggingface.co/intfloat/multilingual-e5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Green tea is rich in antioxidants and can boost brain function.\\n', 'Grüner Tee ist reich an Antioxidantien und kann die Gehirnfunktion verbessern.\\n', 'El té verde es rico en antioxidantes y puede mejorar la función cerebral.\\n', 'Le thé vert est riche en antioxydants et peut stimuler la fonction cérébrale.\\n', '绿茶富含抗氧化剂，可以提高大脑功能。\\n', '緑茶は抗酸化物質が豊富で、脳の機能を向上させることができます。\\n', 'Il tè verde è ricco di antiossidanti e può migliorare la funzione cerebrale.']\n"
     ]
    }
   ],
   "source": [
    "# load some example sentences in different languages\n",
    "with open(\"language-examples.txt\", \"r\") as f:\n",
    "    examples = f.readlines()\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar sentence i -> Green tea is rich in antioxidants and can boost brain function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# downloading and using a multilingual E5 model for embedding purposes and\n",
    "# implement super simple similarity search\n",
    "\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/multilingual-e5-small\")\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "embeddings = [model.encode(sent) for sent in examples]\n",
    "base_vector = base_vector = model.encode(\"Voglio saperne di più sul tè verde\") # I want to learn more about green tea in italian\n",
    "\n",
    "# Calculate cosine similarity for each vector in the list\n",
    "similarities = [cosine_similarity(np.array(vector), np.array(base_vector)) for vector in embeddings]\n",
    "\n",
    "# Find the index of the most similar vector\n",
    "most_similar_index = np.argmax(similarities)\n",
    "\n",
    "print(f\"The most similar sentence i -> {examples[most_similar_index]}\") # should return our Italian example sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more really good embedding models, you can visit the text embedding leaderboard: https://huggingface.co/spaces/mteb/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![leaderboard](leaderboard.png \"HuggingFace embedding model leaderboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling private or sensitive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text data is often filled with personal information. Finding and removing it is not an easy task! Especially in the context of RAG, we don't want to give any private information to third parties. Luckily there are ways to use smaller, lightweight models locally to do some of the work for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides splitting our texts, we can also use SpaCy to find the names of people in our texts. SpaCy offers models in many different languages and can be used for many extraction tasks like this!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"My name is James Bond.\" has name -> \"[('name', 3, 5)]\"\n",
      "text: \"Harry met Jane on a sunny afternoon.\" has name -> \"[('name', 0, 1), ('name', 2, 3)]\"\n",
      "text: \"Say my name.\" doesn't have name\n"
     ]
    }
   ],
   "source": [
    "# detect and replace names of people using spacy\n",
    "import spacy\n",
    "from typing import List, Tuple\n",
    "\n",
    "def person_extraction(text: str, extraction_keyword: str) -> List[Tuple[str, int]]:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    name_positions = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            name_positions.append((extraction_keyword, ent.start, ent.end))\n",
    "    return name_positions\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation\n",
    "\n",
    "def example_integration():\n",
    "    texts = [\"My name is James Bond.\", \"Harry met Jane on a sunny afternoon.\", \"Say my name.\"]\n",
    "    extraction_keyword = \"name\"\n",
    "    for text in texts:\n",
    "        found = person_extraction(text, extraction_keyword)\n",
    "        if found:\n",
    "            print(f\"text: \\\"{text}\\\" has {extraction_keyword} -> \\\"{found}\\\"\")\n",
    "        else:\n",
    "            print(f\"text: \\\"{text}\\\" doesn't have {extraction_keyword}\")\n",
    "\n",
    "example_integration()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Named Entity Extraction with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part you will need a Hugging Face account and API key: https://huggingface.co/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leo/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ORG', 0, 1), ('MISC', 3, 4)]\n",
      "[('PER', 0, 2), ('LOC', 6, 7)]\n"
     ]
    }
   ],
   "source": [
    "# detect and replace names of people using open-source transformer models\n",
    "import requests\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "def bert_ner_extraction(text, api_key):\n",
    "      headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
    "      data = {\"inputs\": text}\n",
    "      try: \n",
    "            response = requests.post(\"https://api-inference.huggingface.co/models/dslim/bert-base-NER\", headers=headers, json=data)\n",
    "            response_json = response.json()\n",
    "            ner_positions = []\n",
    "\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            doc = nlp(text)\n",
    "\n",
    "            for item in response_json:\n",
    "                  start = item[\"start\"]\n",
    "                  end = item[\"end\"]\n",
    "                  span = doc.char_span(start, end, alignment_mode=\"expand\")\n",
    "                  ner_positions.append((item[\"entity_group\"], span.start, span.end))\n",
    "            return ner_positions\n",
    "      except Exception as e: \n",
    "            return f\"That didn't work. Did you provide a valid API key? Go error: {e} and message: {response_json}\"\n",
    "\n",
    "# ↑ necessary bricks function \n",
    "# -----------------------------------------------------------------------------------------\n",
    "# ↓ example implementation \n",
    "\n",
    "def example_integration():\n",
    "      hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "      texts = [\"Apple announces new iPhone.\", \"Angela Merkel was the chancellor of Germany.\"]\n",
    "      for text in texts:\n",
    "            output = bert_ner_extraction(text, api_key=hf_api_key)\n",
    "            print(output)\n",
    "\n",
    "example_integration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
